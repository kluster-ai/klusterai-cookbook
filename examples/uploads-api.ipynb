{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-heading",
   "metadata": {},
   "source": [
    "# Large file uploads with kluster.ai API\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kluster-ai/klusterai-cookbook/blob/main/examples/uploads-api.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-description",
   "metadata": {},
   "source": [
    "When working with large datasets for AI model training or batch inference, you may need to upload files several gigabytes in size. For these scenarios, [kluster.ai](https://www.kluster.ai/) provides a multipart upload API that allows you to split your large files into smaller chunks and upload them efficiently.\n",
    "\n",
    "This tutorial demonstrates how to:\n",
    "1. Split a large file into multiple parts.\n",
    "2. Upload each part using the kluster.ai uploads API.\n",
    "3. Complete the upload process to create a usable File object.\n",
    "4. Use the uploaded file for a batch inference job.\n",
    "\n",
    "The example uses a large JSONL file for batch inference with a language model, but this approach can be adapted for any large file upload scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before getting started, ensure you have the following:\n",
    "\n",
    "- **A kluster.ai account**: Sign up on the [kluster.ai platform](https://platform.kluster.ai/signup) if you don't have one.\n",
    "- **A kluster.ai API key**: After signing in, go to the [**API Keys**](https://platform.kluster.ai/apikeys) section and create a new key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-heading",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-description",
   "metadata": {},
   "source": [
    "Let's start by installing the necessary libraries and setting up our environment. We'll use the OpenAI Python library to interact with the kluster.ai API (since kluster.ai API is OpenAI compatible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-key-setup",
   "metadata": {},
   "source": [
    "In this notebook, we'll use Python's `getpass` module to input the API key securely. After execution, please provide your unique kluster.ai API key (ensure no spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "get-api-key",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your kluster.ai API key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your kluster.ai API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-description",
   "metadata": {},
   "source": [
    "Now, let's import all the necessary libraries that we'll use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client-init",
   "metadata": {},
   "source": [
    "Initialize the OpenAI client by pointing it to the kluster.ai endpoint and passing your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "setup-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.kluster.ai/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-data-heading",
   "metadata": {},
   "source": [
    "## Sample data creation (optional)\n",
    "\n",
    "If you don't already have a large JSONL file for testing, you can create one using the code below. This will generate a synthetic dataset of text prompts that we'll use for batch inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sample-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample file 'data/sample_large_file.jsonl' with 1000 prompts (0.34 MB)\n"
     ]
    }
   ],
   "source": [
    "def create_sample_jsonl(filename, num_samples=1000):\n",
    "    \"\"\"Create a sample JSONL file with text prompts for batch inference.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    topics = [\n",
    "        \"climate change\", \"renewable energy\", \"space exploration\", \"quantum computing\",\n",
    "        \"artificial intelligence\", \"biodiversity\", \"ocean conservation\", \"sustainable agriculture\",\n",
    "        \"future of transportation\", \"advanced materials\"\n",
    "    ]\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(num_samples):\n",
    "            topic = topics[i % len(topics)]\n",
    "            request = {\n",
    "                \"custom_id\": f\"sample-{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    # Feel free to swap out the model with any of kluster's supported models\n",
    "                    # You can also use your own kluster.ai dedicated deployment after launching one\n",
    "                    # And specifying the model ID as the model parameter. \n",
    "                    # For more details, see https://docs.kluster.ai/get-started/dedicated-deployments/\n",
    "                    \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise information.\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"Explain {topic} in one short paragraph\"}\n",
    "                    ],\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(request) + '\\n')\n",
    "    \n",
    "    file_size = os.path.getsize(filename)\n",
    "    print(f\"Created sample file '{filename}' with {num_samples} prompts ({file_size/1024/1024:.2f} MB)\")\n",
    "    return filename, file_size\n",
    "\n",
    "# Create a sample JSONL file with 1000 prompts\n",
    "sample_file, file_size = create_sample_jsonl('data/sample_large_file.jsonl', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multipart-heading",
   "metadata": {},
   "source": [
    "## Multipart upload process\n",
    "\n",
    "Now, let's implement the multipart upload process. We'll break this down into several steps:\n",
    "\n",
    "1. Create the [Upload object](https://docs.kluster.ai/api-reference/reference/#upload-object).\n",
    "2. Split the file into chunks and upload each chunk as a part.\n",
    "3. Complete the upload process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-heading",
   "metadata": {},
   "source": [
    "### Create the upload object\n",
    "\n",
    "First, we need to create an [Upload object](https://docs.kluster.ai/api-reference/reference/#upload-object) that will serve as a container for all the parts we're about to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "create-upload",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created upload with ID: 6806aa95647c3680875b6339\n",
      "Upload will expire at: 2025-04-21 16:29:09\n"
     ]
    }
   ],
   "source": [
    "def create_upload(client, filename, file_size, purpose=\"batch\", mime_type=\"application/jsonl\"):\n",
    "    \"\"\"Create an Upload object to which we can add parts.\"\"\"\n",
    "    upload = client.uploads.create(\n",
    "        purpose=purpose,\n",
    "        filename=os.path.basename(filename),\n",
    "        bytes=file_size,\n",
    "        mime_type=mime_type\n",
    "    )\n",
    "    print(f\"Created upload with ID: {upload.id}\")\n",
    "    print(f\"Upload will expire at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(upload.expires_at))}\")\n",
    "    return upload\n",
    "\n",
    "# Get the file you want to upload\n",
    "file_to_upload = sample_file\n",
    "file_size = os.path.getsize(file_to_upload)\n",
    "\n",
    "# Create the upload object\n",
    "upload = create_upload(client, file_to_upload, file_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-heading",
   "metadata": {},
   "source": [
    "### Split the file and upload parts\n",
    "\n",
    "Now, we'll split the file into chunks and upload each chunk as a part. According to the documentation, each part can be at most 64 MB, so we'll make sure our chunks don't exceed that limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "upload-parts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file in 2 parts (part size: 0.17 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading parts:  50%|██████████████              | 1/2 [00:00<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1/2 uploaded with ID: 6806aa968967dabeabba9ebd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading parts: 100%|████████████████████████████| 2/2 [00:00<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2/2 uploaded with ID: 6806aa966f520bb3f2023acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def split_and_upload_parts(client, upload_id, file_path, num_parts=2):\n",
    "    \"\"\"Split a file into a specific number of chunks and upload each chunk as a part.\"\"\"\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    part_size = math.ceil(file_size / num_parts)\n",
    "    \n",
    "    print(f\"Uploading file in {num_parts} parts (part size: {part_size/1024/1024:.2f} MB)\")\n",
    "    \n",
    "    parts = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for i in tqdm(range(num_parts), desc=\"Uploading parts\"):\n",
    "            # Read a chunk of the file\n",
    "            chunk = f.read(part_size)\n",
    "            \n",
    "            # Create a temporary file for the chunk\n",
    "            temp_filename = f\"part_{i}.tmp\"\n",
    "            with open(temp_filename, 'wb') as temp_f:\n",
    "                temp_f.write(chunk)\n",
    "            \n",
    "            # Upload the part\n",
    "            with open(temp_filename, 'rb') as temp_f:\n",
    "                part = client.uploads.parts.create(\n",
    "                    upload_id=upload_id,\n",
    "                    data=temp_f\n",
    "                )\n",
    "                parts.append(part)\n",
    "                # Log the part ID\n",
    "                print(f\"Part {i+1}/{num_parts} uploaded with ID: {part.id}\")\n",
    "            \n",
    "            # Clean up the temporary file\n",
    "            os.remove(temp_filename)\n",
    "    \n",
    "    return parts\n",
    "\n",
    "# For demonstration purposes, we'll use a smaller part size\n",
    "parts = split_and_upload_parts(client, upload.id, file_to_upload, num_parts=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md5-calc",
   "metadata": {},
   "source": [
    "Let's prepare to complete the upload process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "calc-md5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File MD5 checksum: 69888e6132fa493024089b877d812c89\n"
     ]
    }
   ],
   "source": [
    "def calculate_md5(file_path):\n",
    "    \"\"\"Calculate the MD5 checksum of a file.\"\"\"\n",
    "    md5_hash = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Read the file in chunks to avoid loading large files into memory\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            md5_hash.update(chunk)\n",
    "    return md5_hash.hexdigest()\n",
    "\n",
    "file_md5 = calculate_md5(file_to_upload)\n",
    "print(f\"File MD5 checksum: {file_md5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-heading",
   "metadata": {},
   "source": [
    "### Complete the upload\n",
    "\n",
    "Finally, we'll complete the upload process by providing the ordered list of part IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complete-upload",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload completed successfully!\n",
      "Status: completed\n",
      "File ID: 6806aa97b2cfbed2561aecf0\n"
     ]
    }
   ],
   "source": [
    "def complete_upload(client, upload_id, parts):\n",
    "    \"\"\"Complete the upload process with the ordered list of part IDs.\"\"\"\n",
    "    part_ids = [part.id for part in parts]\n",
    "    \n",
    "    params = {\n",
    "        \"upload_id\": upload_id,\n",
    "        \"part_ids\": part_ids\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        completed_upload = client.uploads.complete(**params)\n",
    "        \n",
    "        print(f\"Upload completed successfully!\")\n",
    "        print(f\"Status: {completed_upload.status}\")\n",
    "        print(f\"File ID: {completed_upload.file.id}\")\n",
    "        return completed_upload\n",
    "    except Exception as e:\n",
    "        print(f\"Error completing upload: {e}\")\n",
    "        raise\n",
    "        \n",
    "# Complete the upload\n",
    "completed_upload = complete_upload(client, upload.id, parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-job-heading",
   "metadata": {},
   "source": [
    "## Use the uploaded file for batch inference\n",
    "\n",
    "Now that we've successfully uploaded our large file, let's use it to create a batch inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create-batch-job",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch job created with ID: 6806aa976cdee70bad145125\n",
      "Status: pre_schedule\n"
     ]
    }
   ],
   "source": [
    "def create_batch_job(client, file_id):\n",
    "    \"\"\"Create a batch job using the uploaded file.\"\"\"\n",
    "    batch_job = client.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch job created with ID: {batch_job.id}\")\n",
    "    print(f\"Status: {batch_job.status}\")\n",
    "    return batch_job\n",
    "\n",
    "# Create a batch job with the file we just uploaded\n",
    "batch_job = create_batch_job(client, completed_upload.file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor-job-heading",
   "metadata": {},
   "source": [
    "### Monitor batch job progress\n",
    "\n",
    "Let's monitor the progress of our batch job. We'll check the status every 10 seconds until the job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "monitor-batch-job",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job completed!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Output file ID: 6806aadc694596f597839fe1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def monitor_batch_job(client, batch_job_id, check_interval=10):\n",
    "    \"\"\"Monitor the progress of a batch job until it's completed.\"\"\"\n",
    "    all_completed = False\n",
    "    \n",
    "    while not all_completed:\n",
    "        all_completed = True\n",
    "        output_lines = []\n",
    "        \n",
    "        updated_job = client.batches.retrieve(batch_job_id)\n",
    "        \n",
    "        if updated_job.status != \"completed\":\n",
    "            all_completed = False\n",
    "            completed = updated_job.request_counts.completed\n",
    "            total = updated_job.request_counts.total\n",
    "            output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\")\n",
    "        else:\n",
    "            output_lines.append(f\"Job completed!\")\n",
    "            output_lines.append(f\"Output file ID: {updated_job.output_file_id}\")\n",
    "        \n",
    "        # Clear the output and display updated status\n",
    "        clear_output(wait=True)\n",
    "        for line in output_lines:\n",
    "            display(line)\n",
    "        \n",
    "        if not all_completed:\n",
    "            time.sleep(check_interval)\n",
    "    \n",
    "    return updated_job\n",
    "\n",
    "# Monitor the batch job progress\n",
    "completed_job = monitor_batch_job(client, batch_job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-heading",
   "metadata": {},
   "source": [
    "### Retrieve and process results\n",
    "\n",
    "Once the batch job is completed, we can retrieve and process the results. Specifically, we’ll download the output file returned by the API and parse each line‑delimited JSON record into Python objects, making it easy to inspect, validate, and visualize every response produced by the batch run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "retrieve-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1000 results\n",
      "\n",
      "Sample of results:\n",
      "\n",
      "Result 1 (Custom ID: sample-0):\n",
      "Climate change is a global phenomenon caused by the increasing levels of greenhouse gases, such as carbon dioxide and methane, in the Earth's atmosphere. These gases trap heat from the sun, leading to a rise in global temperatures, changes in weather patterns, and more frequent extreme events like heatwaves, droughts, and storms. This is primarily driven by human activities like burning fossil fuels, deforestation, and industrial processes, which release large amounts of greenhouse gases into the atmosphere.\n",
      "\n",
      "Result 2 (Custom ID: sample-1):\n",
      "Renewable energy is derived from natural resources that can be replenished over time, such as sunlight, wind, water, and geothermal heat. Unlike fossil fuels, renewable energy sources are sustainable and non-polluting, producing electricity or heat with minimal environmental impact. Examples of renewable energy include solar power, wind power, hydroelectric power, and biomass energy, which can be harnessed to power homes, industries, and transportation systems.\n",
      "\n",
      "Result 3 (Custom ID: sample-2):\n",
      "Space exploration is the ongoing effort to explore the universe beyond Earth's atmosphere, seeking to understand the mysteries of space and the planets within it. Through various missions, space agencies and private companies have successfully landed rovers on Mars, sent probes to distant planets and asteroids, and even established temporary human settlements in space, such as the International Space Station. The goal of space exploration is to expand our knowledge of the cosmos, unlock new technologies, and potentially one day establish a human presence beyond Earth.\n",
      "\n",
      "Result 4 (Custom ID: sample-3):\n",
      "Quantum computing is a revolutionary technology that uses the principles of quantum mechanics to perform calculations and operations on data. Unlike traditional computers, which use bits to represent 0s and 1s, quantum computers use quantum bits or qubits, which can exist in multiple states simultaneously, allowing them to process vast amounts of information exponentially faster and more efficiently.\n",
      "\n",
      "Result 5 (Custom ID: sample-4):\n",
      "Artificial intelligence (AI) refers to the development of computer systems that can perform tasks typically requiring human intelligence, such as learning, problem-solving, decision-making, and perception. AI systems use algorithms and data to analyze and understand complex information, enabling them to make predictions, classify objects, and automate tasks, ultimately simulating human-like behavior and intelligence.\n"
     ]
    }
   ],
   "source": [
    "def retrieve_batch_results(client, job):\n",
    "    \"\"\"Retrieve and parse the results of a completed batch job.\"\"\"\n",
    "    if job.status != \"completed\":\n",
    "        print(f\"Job is not completed yet. Current status: {job.status}\")\n",
    "        return None\n",
    "    \n",
    "    result_file_id = job.output_file_id\n",
    "    result = client.files.content(result_file_id).content\n",
    "    \n",
    "    # Parse JSON results\n",
    "    if isinstance(result, bytes):\n",
    "        result = result.decode('utf-8')\n",
    "        \n",
    "    json_strings = result.strip().split('\\n')\n",
    "    json_objects = []\n",
    "    \n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "    \n",
    "    return json_objects\n",
    "\n",
    "# Retrieve and parse the batch job results\n",
    "results = retrieve_batch_results(client, completed_job)\n",
    "\n",
    "# Display a sample of the results\n",
    "if results:\n",
    "    print(f\"Retrieved {len(results)} results\")\n",
    "    \n",
    "    # Now display the results with a more streamlined approach\n",
    "    print(\"\\nSample of results:\")\n",
    "    for i, result in enumerate(results[:5]):\n",
    "        try:\n",
    "            # Extract the content and ID based on the structure we now know\n",
    "            content = result[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            custom_id = result.get(\"custom_id\", f\"unknown-{i}\")\n",
    "                \n",
    "            print(f\"\\nResult {i+1} (Custom ID: {custom_id}):\\n{content}\")\n",
    "        except (KeyError, IndexError, TypeError) as e:\n",
    "            print(f\"Error processing result {i+1}: {e}\")\n",
    "            # Print only the keys to understand structure, not all content\n",
    "            print(f\"Result keys: {list(result.keys())}\")\n",
    "            if \"response\" in result:\n",
    "                print(f\"Response keys: {list(result['response'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cancel-upload-heading",
   "metadata": {},
   "source": [
    "## Handle failed uploads\n",
    "\n",
    "Sometimes, uploads may fail for various reasons. In such cases, you may want to cancel the upload and start over. Here's how to cancel an upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cancel-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_upload(client, upload_id):\n",
    "    \"\"\"Cancel an upload that's in progress or has failed.\"\"\"\n",
    "    cancelled_upload = client.uploads.cancel(upload_id=upload_id)\n",
    "    \n",
    "    print(f\"Upload cancelled successfully!\")\n",
    "    print(f\"Status: {cancelled_upload.status}\")\n",
    "    return cancelled_upload\n",
    "\n",
    "# This is just for demonstration - we won't actually cancel our upload\n",
    "# cancel_upload(client, upload.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-heading",
   "metadata": {},
   "source": [
    "## Best practices and tips\n",
    "\n",
    "Here are some best practices and tips for using the kluster.ai Uploads API effectively:\n",
    "\n",
    "- **Optimal part size**: Choose an appropriate part size based on your network conditions. While the API allows parts up to 64 MB, you might want to use smaller parts if your network is unstable.\n",
    "- **Parallel uploads**: For very large files, consider uploading parts in parallel to speed up the process.\n",
    "- **Error handling**: Implement proper error handling and retries for failed part uploads.\n",
    "- **Upload expiration**: Remember that uploads expire after an hour, so make sure to complete the upload within that time frame.\n",
    "- **Cleanup**: Always clean up temporary files created during the upload process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-section",
   "metadata": {},
   "source": [
    "## Advanced: implement parallel uploads\n",
    "\n",
    "For very large files, uploading parts sequentially might take a long time. You can implement a version that uploads parts in parallel using Python's `concurrent.futures` module. This approach creates a thread pool that allows multiple parts to be uploaded simultaneously, significantly reducing the total upload time for large files over high-bandwidth connections.\n",
    "\n",
    "The implementation uses the `ThreadPoolExecutor` to manage a pool of worker threads that process uploads concurrently. Each chunk is written to a temporary file, uploaded to the server, and then the temporary file is cleaned up. The code carefully tracks each part's original position to ensure they're properly ordered when completing the upload, regardless of which part finishes uploading first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "parallel-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import tempfile\n",
    "\n",
    "def upload_part(client, upload_id, part_data, part_index):\n",
    "    \"\"\"Upload a single part and return the part object.\"\"\"\n",
    "    # Create a temporary file for the chunk\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp_f:\n",
    "        temp_filename = temp_f.name\n",
    "        temp_f.write(part_data)\n",
    "    \n",
    "    try:\n",
    "        # Upload the part\n",
    "        with open(temp_filename, 'rb') as temp_f:\n",
    "            part = client.uploads.parts.create(\n",
    "                upload_id=upload_id,\n",
    "                data=temp_f\n",
    "            )\n",
    "        return part, part_index\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        os.remove(temp_filename)\n",
    "\n",
    "def parallel_upload_parts(client, upload_id, file_path, num_parts=2, max_workers=2):\n",
    "    \"\"\"Split a file into a specific number of chunks and upload parts in parallel.\"\"\"\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    part_size = math.ceil(file_size / num_parts)\n",
    "    \n",
    "    print(f\"Uploading file in {num_parts} parts using up to {max_workers} parallel workers\")\n",
    "    \n",
    "    # Read all chunks from the file\n",
    "    chunks = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for i in range(num_parts):\n",
    "            chunk = f.read(part_size)\n",
    "            if chunk:  # Ensure we don't add empty chunks\n",
    "                chunks.append(chunk)\n",
    "    \n",
    "    # Upload parts in parallel\n",
    "    parts = [None] * len(chunks)  # Pre-allocate list to maintain order\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all upload tasks\n",
    "        future_to_index = {}\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            future = executor.submit(upload_part, client, upload_id, chunk, i)\n",
    "            future_to_index[future] = i\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(future_to_index.keys())):\n",
    "            try:\n",
    "                part, index = future.result()\n",
    "                parts[index] = part\n",
    "                print(f\"Part {index+1}/{len(chunks)} uploaded with ID: {part.id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading part: {e}\")\n",
    "    \n",
    "    # Ensure all parts were uploaded successfully\n",
    "    if None in parts:\n",
    "        raise Exception(\"Some parts failed to upload\")\n",
    "    \n",
    "    return parts\n",
    "    \n",
    "# To use parallel uploads, replace the call to split_and_upload_parts with:\n",
    "# parts = parallel_upload_parts(client, upload.id, file_to_upload, num_parts=2, max_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4009270-6cf0-4b19-98b0-ae262d1b626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## PARALLEL UPLOAD DEMONSTRATION ##\n",
      "\n",
      "Creating a new upload for parallel demonstration...\n",
      "Created upload with ID: 6806aadfb2cfbed2561aeea9\n",
      "Upload will expire at: 2025-04-21 16:30:23\n",
      "\n",
      "Uploading parts in parallel...\n",
      "Uploading file in 2 parts using up to 2 parallel workers\n",
      "Part 1/2 uploaded with ID: 6806aae0fd88d31a6f21d200\n",
      "Part 2/2 uploaded with ID: 6806aae06cdee70bad1452ac\n",
      "\n",
      "Completing the parallel upload...\n",
      "Upload completed successfully!\n",
      "Status: completed\n",
      "File ID: 6806aae0d6646988da98c092\n",
      "\n",
      "Parallel upload demonstration completed!\n",
      "Parallel uploaded file ID: 6806aae0d6646988da98c092\n",
      "Note: This file was uploaded just for demonstration purposes.\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Demonstrating Parallel Uploads\n",
    "\n",
    "print(\"\\n\\n## PARALLEL UPLOAD DEMONSTRATION ##\\n\")\n",
    "\n",
    "# First, create a new upload object for our parallel demo\n",
    "print(\"Creating a new upload for parallel demonstration...\")\n",
    "parallel_upload = create_upload(client, file_to_upload, file_size)\n",
    "\n",
    "# Now use the parallel upload function instead of the sequential one\n",
    "print(\"\\nUploading parts in parallel...\")\n",
    "parallel_parts = parallel_upload_parts(client, parallel_upload.id, file_to_upload, num_parts=2, max_workers=2)\n",
    "\n",
    "# Complete the parallel upload\n",
    "print(\"\\nCompleting the parallel upload...\")\n",
    "parallel_completed_upload = complete_upload(client, parallel_upload.id, parallel_parts)\n",
    "\n",
    "print(\"\\nParallel upload demonstration completed!\")\n",
    "print(f\"Parallel uploaded file ID: {parallel_completed_upload.file.id}\")\n",
    "\n",
    "# We won't use this file for further processing, it was just for demonstration\n",
    "print(\"Note: This file was uploaded just for demonstration purposes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-heading",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've learned how to:\n",
    "\n",
    "1. Create an Upload object using the kluster.ai API.\n",
    "2. Split a large file into smaller parts and upload each part.\n",
    "3. Complete the upload process to create a usable File object.\n",
    "4. Use the uploaded file for a batch inference job.\n",
    "5. Monitor the batch job progress and retrieve results.\n",
    "\n",
    "This approach allows you to efficiently upload and process large datasets with kluster.ai, making it an invaluable tool for your AI workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
