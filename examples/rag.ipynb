{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566e7e41",
   "metadata": {},
   "source": [
    "# Retrieval-augmented generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a77d9",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kluster-ai/klusterai-cookbook/blob/main/examples/rag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566fe2a",
   "metadata": {},
   "source": [
    "There are use cases when models struggle to complete tasks due to a lack of specialized knowledge. This could lead to issues like hallucinations, where models generate incorrect or fabricated information.\n",
    "To help mitigate this, **Retrieval-Augmented Generation (RAG)** allows the model to pull real-time data from specified sources.\n",
    "\n",
    "This tutorial demonstrates how to use the <a href=\"https://docs.kluster.ai/api-reference/reference/#/http/api-endpoints/embeddings/v1-embeddings-post\" target=\"_blank\">embeddings endpoint</a> in a RAG pipeline with PDF document support using <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/\" target=\"_blank\">LlamaIndex</a> to handle the data and retrieval. \n",
    "\n",
    "\n",
    "**Models:**\n",
    "- **Embeddings**: Leveraging the <a href=\"https://platform.kluster.ai/models\" target=\"_blank\">BAAI/bge-m3</a> model.\n",
    "- **Language Model (LLM) for querying**: For this example we use <a href=\"https://platform.kluster.ai/playground?model=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" target=\"_blank\">Llama-4-Maverick-17B.</a> You can also choose any other model from the <a href=\"https://platform.kluster.ai/models\" target=\"_blank\">available models list.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82108a06",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before getting started, ensure you have the following:\n",
    "\n",
    "- **A kluster.ai account**: Sign up on the <a href=\"https://platform.kluster.ai/signup\" target=\"_blank\">kluster.ai platform</a> if you don't have one.\n",
    "- **A kluster.ai API key**: After signing in, go to the <a href=\"https://platform.kluster.ai/apikeys\" target=\"_blank\">**API Keys**</a> section and create a new key. For detailed instructions, check out the <a href=\"/get-started/get-api-key/\" target=\"_blank\">Get an API key</a> guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43438e0a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this notebook, we'll use Python's `getpass` module to safely input the key. Provide your unique kluster.ai API key (ensure there are no spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89313886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your kluster.ai API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages, including the PDF reader for LlamaIndex\n",
    "\n",
    "%pip install llama-index llama-index-llms-openai-like llama-index-embeddings-openai-like llama-index-readers-file requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "base_url=\"https://api.kluster.ai/v1\"\n",
    "\n",
    "# Configure kluster.ai client\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4865a",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings are numerical representations of text that capture their meaning in a format computers can understand.\n",
    "\n",
    "Think of embeddings as coordinates in a high-dimensional space, where similar meanings are placed closer together, which allows RAG systems to measure how related different pieces of text are, making it easier to find relevant information quickly.\n",
    "\n",
    "This section demonstrates how to **generate embeddings** directly using the `BAAI/bge-m3` model and the end result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f291bbb",
   "metadata": {},
   "source": [
    "### Generate embeddings\n",
    "\n",
    "Generating embeddings for RAG systems is crucial because embeddings capture the semantic meaning of text, enabling the efficient retrieval of relevant information from a knowledge base. \n",
    "\n",
    "Let's convert a sample text to embeddings and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0e710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: 'The capital of Argentina is Buenos Aires. It is known for its tango music and dance, as well as its vibrant nightlife.'\n",
      "Model used: BAAI/bge-m3\n",
      "Embedding dimensions: 1024\n",
      "\n",
      "First ten dimensions of the embedding vector:\n",
      "[0.054107666015625, 0.01416778564453125, -0.0236663818359375, 0.039306640625, -0.039337158203125, 0.0098724365234375, -0.013946533203125, 0.037872314453125, -0.057281494140625, 0.01058197021484375]\n",
      "\n",
      "Token usage: 29 tokens\n"
     ]
    }
   ],
   "source": [
    "# Generate embedding for the example text about Buenos Aires\n",
    "sample_text = \"The capital of Argentina is Buenos Aires. It is known for its tango music and dance, as well as its vibrant nightlife.\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"BAAI/bge-m3\",\n",
    "    input=sample_text,\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "# Print the first ten dimensions of the embedding vector\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"Model used: {response.model}\")\n",
    "print(f\"Embedding dimensions: {len(response.data[0].embedding)}\")\n",
    "print(\"\\nFirst ten dimensions of the embedding vector:\")\n",
    "print(response.data[0].embedding[:10])\n",
    "\n",
    "# Show token usage information\n",
    "print(f\"\\nToken usage: {response.usage.prompt_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5l5ebx93nza",
   "metadata": {},
   "source": [
    "### Batch embeddings\n",
    "\n",
    "When building RAG systems, you often need to process multiple documents or text chunks. Instead of making individual API calls for each piece of text, batch embeddings allow you to process multiple texts in a single request, significantly improving efficiency and reducing latency.\n",
    "\n",
    "The embeddings endpoint accepts an array of strings, processing up to 2048 individual text inputs in one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "k7derk0u5w",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts processed: 5\n",
      "Number of embeddings returned: 5\n",
      "Total tokens used: 64\n",
      "\n",
      "Text 1: 'The capital of Argentina is Buenos Aires....'\n",
      "Embedding dimensions: 1024\n",
      "First five values: [0.037200927734375, 0.022430419921875, -0.032928466796875, 0.0226593017578125, -0.046539306640625]\n",
      "\n",
      "Text 2: 'Paris is known for the Eiffel Tower and its romant...'\n",
      "Embedding dimensions: 1024\n",
      "First five values: [-0.0014429092407226562, 0.0240478515625, -0.0137481689453125, 0.04278564453125, -0.0023708343505859375]\n",
      "\n",
      "Text 3: 'Tokyo is the most populous metropolitan area in th...'\n",
      "Embedding dimensions: 1024\n",
      "First five values: [-0.0001596212387084961, 0.0217742919921875, -0.0133056640625, 0.0304412841796875, 0.00852203369140625]\n",
      "\n",
      "Text 4: 'London has a rich history dating back to Roman tim...'\n",
      "Embedding dimensions: 1024\n",
      "First five values: [-0.0438232421875, 0.03497314453125, -0.021148681640625, 0.0021762847900390625, -0.01172637939453125]\n",
      "\n",
      "Text 5: 'New York City is often called the Big Apple....'\n",
      "Embedding dimensions: 1024\n",
      "First five values: [0.0038509368896484375, 0.03302001953125, -0.06243896484375, 0.0157318115234375, -0.0172882080078125]\n"
     ]
    }
   ],
   "source": [
    "# Example: Processing multiple texts in a single batch\n",
    "batch_texts = [\n",
    "    \"The capital of Argentina is Buenos Aires.\",\n",
    "    \"Paris is known for the Eiffel Tower and its romantic atmosphere.\",\n",
    "    \"Tokyo is the most populous metropolitan area in the world.\",\n",
    "    \"London has a rich history dating back to Roman times.\",\n",
    "    \"New York City is often called the Big Apple.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all texts in one API call\n",
    "batch_response = client.embeddings.create(\n",
    "    model=\"BAAI/bge-m3\",\n",
    "    input=batch_texts,\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "print(f\"Number of texts processed: {len(batch_texts)}\")\n",
    "print(f\"Number of embeddings returned: {len(batch_response.data)}\")\n",
    "print(f\"Total tokens used: {batch_response.usage.prompt_tokens}\")\n",
    "\n",
    "# Verify the embeddings are returned in the same order\n",
    "for i, text in enumerate(batch_texts):\n",
    "    print(f\"\\nText {i+1}: '{text[:50]}...'\")\n",
    "    print(f\"Embedding dimensions: {len(batch_response.data[i].embedding)}\")\n",
    "    print(f\"First five values: {batch_response.data[i].embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ziqmmbsg1oj",
   "metadata": {},
   "source": [
    "### Performance comparison: single vs batch\n",
    "\n",
    "To demonstrate the practical benefits of batch processing, let's compare the performance of individual API calls versus kluster.ai's batch processing using the same set of texts. This comparison will show the time savings and efficiency gains you can expect when implementing batch embeddings in production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0otvoy0s56ap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Individual API calls\n",
      "Time taken: 2.81 seconds\n",
      "Number of API calls: 5\n",
      "\n",
      "Method 2: Batch API call\n",
      "Time taken: 0.76 seconds\n",
      "Number of API calls: 1\n",
      "\n",
      "Batch processing is approximately 3.7x faster!\n",
      "Time saved: 2.05 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Test texts for performance comparison\n",
    "test_texts = [\n",
    "    \"Machine learning is transforming industries.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning models require significant computational resources.\",\n",
    "    \"Transfer learning allows models to apply knowledge from one domain to another.\",\n",
    "    \"Embeddings capture semantic meaning in numerical form.\"\n",
    "]\n",
    "\n",
    "# Method 1: Individual API calls (not recommended for production)\n",
    "print(\"Method 1: Individual API calls\")\n",
    "start_time = time.time()\n",
    "individual_embeddings = []\n",
    "\n",
    "for text in test_texts:\n",
    "    response = client.embeddings.create(\n",
    "        model=\"BAAI/bge-m3\",\n",
    "        input=text,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    individual_embeddings.append(response.data[0].embedding)\n",
    "\n",
    "individual_time = time.time() - start_time\n",
    "print(f\"Time taken: {individual_time:.2f} seconds\")\n",
    "print(f\"Number of API calls: {len(test_texts)}\")\n",
    "\n",
    "# Method 2: Batch API call (recommended)\n",
    "print(\"\\nMethod 2: Batch API call\")\n",
    "start_time = time.time()\n",
    "\n",
    "batch_response = client.embeddings.create(\n",
    "    model=\"BAAI/bge-m3\",\n",
    "    input=test_texts,\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "batch_time = time.time() - start_time\n",
    "print(f\"Time taken: {batch_time:.2f} seconds\")\n",
    "print(f\"Number of API calls: 1\")\n",
    "\n",
    "# Performance improvement\n",
    "improvement = (individual_time / batch_time)\n",
    "print(f\"\\nBatch processing is approximately {improvement:.1f}x faster!\")\n",
    "print(f\"Time saved: {individual_time - batch_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0szgjthcbzkd",
   "metadata": {},
   "source": [
    "In practice, when building RAG systems with frameworks like LlamaIndex, the embedding batching is often handled automatically. Let's see how this works when we process PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91011ad2",
   "metadata": {},
   "source": [
    "## Build a RAG system with PDF documents\n",
    "\n",
    "Now that we understand how embeddings work, let's build a complete RAG system using a real PDF document. This section demonstrates how to:\n",
    "\n",
    "- Download and load a PDF document.\n",
    "- Split the document into manageable chunks.\n",
    "- Convert those chunks into embeddings using kluster.ai.\n",
    "- Create a searchable knowledge base.\n",
    "- Query the system to retrieve relevant information.\n",
    "\n",
    "We'll use a research paper about polar bears as our knowledge source, showing how RAG can help answer specific questions about document content that wouldn't be in the LLM's training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be42a3",
   "metadata": {},
   "source": [
    "### Download the document\n",
    "For this exercise, we use a large PDF file, but you can adapt this to your needs.\n",
    "\n",
    "The files used will serve as the LLM's knowledge base. \n",
    "\n",
    "Download the PDF and store it in the `sample_pdfs` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b17b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sample PDF to sample_pdfs/polar_bears.pdf...\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Create a directory for PDFs if it doesn't exist\n",
    "pdf_dir = \"sample_pdfs\"\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "# Download a sample PDF about Polar Bears (you can replace with your own PDFs)\n",
    "sample_pdf_url = \"https://portals.iucn.org/library/sites/library/files/documents/SSC-OP-007.pdf\"\n",
    "pdf_path = os.path.join(pdf_dir, \"polar_bears.pdf\")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Downloading sample PDF to {pdf_path}...\")\n",
    "    urllib.request.urlretrieve(sample_pdf_url, pdf_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Sample PDF already exists at {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8dd11",
   "metadata": {},
   "source": [
    "### Load the document\n",
    "\n",
    "Now it's time to load the 115-page PDF document into memory.\n",
    "\n",
    "This example leverages <a href=\"https://docs.llamaindex.ai/en/stable/examples/data_connectors/simple_directory_reader/\" target=\"_blank\">LlamaIndex</a> `SimpleDirectoryReader` as a data connector. Pass in an input directory or a list of files, and it selects the best file reader based on the file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458616cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from sample_pdfs...\n",
      "Loaded 115 document(s) from PDF file\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary document loader from llama_index\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load documents from the PDF file\n",
    "print(f\"Loading PDF from {pdf_dir}...\")\n",
    "pdf_reader = SimpleDirectoryReader(input_dir=pdf_dir)\n",
    "documents = pdf_reader.load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from PDF file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42ac97",
   "metadata": {},
   "source": "## Set up RAG\n\nIf the entire PDF document is sent to the LLM, it would consume the entire context window with no success.\n\nTo address this, the document needs to be divided into smaller pieces called chunks. <a href=\"https://docs.llamaindex.ai/en/stable/\" target=\"_blank\">LlamaIndex</a> provides an efficient way to accomplish this.\n\n- **LLM**: The model responsible for generating the responses from the knowledge base.\n- **Embedding model**: The model used to convert text into vectors (embeddings), enabling semantic similarity searches.\n- **Chunking parameters**: Defines how documents are split into smaller chunks (nodes) for indexing:\n    - `chunk_size`: The size of each chunk (in tokens).\n    - `chunk_overlap`: The number of tokens overlapping between consecutive chunks.\n\nLlamaIndex provides OpenAI-compatible interfaces that allow you to connect to any API that follows the OpenAI format. Since kluster.ai uses OpenAI-compatible endpoints, we use:\n\n- **`OpenAILike`**: A wrapper that adapts kluster.ai's chat completion API to work with LlamaIndex's LLM interface\n- **`OpenAILikeEmbedding`**: A wrapper that adapts kluster.ai's embeddings API to work with LlamaIndex's embedding interface\n\nThis approach allows you to use kluster.ai models seamlessly within LlamaIndex without requiring custom integration code.\n\nTo set up with kluster.ai, configure `OpenAILike` for the LLM and `OpenAILikeEmbedding` for the embedding model."
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52d3b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kluster.ai embedding model configured.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.embeddings.openai_like import OpenAILikeEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Set up the llamaIndex client with kluster.ai\n",
    "llm = OpenAILike(\n",
    "    model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    api_base=base_url,\n",
    "    api_key=api_key,\n",
    "    is_chat_model=True\n",
    ")\n",
    "\n",
    "# Set up the embedding model\n",
    "embed_model = OpenAILikeEmbedding(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    api_base=base_url,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Set the global settings for LlamaIndex\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512  # Set chunk size for document splitting\n",
    "Settings.chunk_overlap = 20  # Set chunk overlap for document splitting\n",
    "\n",
    "print(\"kluster.ai embedding model configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc04f1",
   "metadata": {},
   "source": "### Create the vector store\n\nBefore building the index, let's quickly clarify a couple of terms:\n\n- **Index**: A searchable structure built from your documents that enables fast similarity search\n- **Vector store**: Stores the embeddings (vector representations) of each document chunk, enabling rapid retrieval\n\nCreating a <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\" target=\"_blank\">VectorStoreIndex</a> combines these concepts â€” it enables your RAG pipeline to efficiently search your PDF and retrieve the most relevant chunks, grounding the LLM's responses in actual document content.\n\n`VectorStoreIndex.from_documents(documents)` takes the PDF and internally breaks these documents into smaller text chunks (Nodes) using settings like `chunk_size` defined earlier. \n \nNext, it converts each chunk into embeddings using the pre-configured embedding model. **LlamaIndex automatically batches these embedding requests** for efficiency, similar to what we demonstrated above.\n\nFinally, it stores these chunks and their embeddings in an in-memory vector store, making the index object ready for efficient similarity searches.\n\nCreate a `VectorStoreIndex` from the PDF document."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390b784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index from PDF document...\n",
      "Index created successfully!\n",
      "Index created successfully!\n",
      "Number of text chunks (nodes) indexed: 320\n",
      "Index ID: 93e6a54a-ca85-44c9-8bac-953c15483f06\n",
      "Vector store type: <class 'llama_index.core.vector_stores.simple.SimpleVectorStore'>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the index from the PDF document\n",
    "print(\"Creating index from PDF document...\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents\n",
    ")\n",
    "# Get the document store from the index\n",
    "docstore = index.docstore\n",
    "\n",
    "# Get the number of nodes (chunks) in the document store\n",
    "num_nodes = len(docstore.docs)\n",
    "\n",
    "print(\"Index created successfully!\")\n",
    "print(f\"Number of text chunks (nodes) indexed: {num_nodes}\")\n",
    "\n",
    "# You can also print the index ID if you're curious\n",
    "print(f\"Index ID: {index.index_id}\")\n",
    "\n",
    "# And the type of vector store being used (by default it's an in-memory SimpleVectorStore)\n",
    "print(f\"Vector store type: {type(index.vector_store)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239a8c0",
   "metadata": {},
   "source": "### Create the query engine\n\nWhen a question is asked, it's first converted into a numerical embedding. The engine then searches the VectorStoreIndex to retrieve the most semantically similar text chunks from the document. Finally, these retrieved chunks (as context) and the original question are given to the LLM to generate a grounded answer.\n\nThe following steps create a query engine and a helper function for direct LLM responses (which we'll use later for comparison)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415d1f1",
   "metadata": {},
   "outputs": [],
   "source": "# Create a query engine for RAG\nquery_engine = index.as_query_engine()\n\n# Function to get a direct response from the LLM without using RAG\ndef get_direct_llm_response(query):\n    \"\"\"Get a response directly from the LLM without using RAG\"\"\"\n    return llm.complete(query).text\n\nprint(\"Query engine created successfully!\")"
  },
  {
   "cell_type": "markdown",
   "id": "8f48f95e",
   "metadata": {},
   "source": [
    "## Test the RAG\n",
    "\n",
    "Now that the RAG system is configured, test it with a query about the PDF document. This demonstration shows how a single RAG query processes:\n",
    "\n",
    "1. **Query processing**: The question is converted into an embedding vector.\n",
    "2. **Chunk retrieval**: The system finds the most relevant document chunks using similarity search.\n",
    "3. **Response generation**: The LLM uses the retrieved chunks as context to answer the question.\n",
    "\n",
    "The following example includes detailed output showing the retrieved chunks and their similarity scores, helping understand how the RAG system selects relevant information from the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50283ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the RAG system with:\n",
      "'Fact check this: <quote> The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears.</quote> If you don't know, say 'I don't know'.'\n",
      "\n",
      "--- Processing RAG query... ---\n",
      "\n",
      "--- Retrieved Context (Source Nodes used by RAG) ---\n",
      "Source Node 1 (Similarity Score: 0.6246):\n",
      "Retrieved Chunk: \"Cape ChurchillWildli...\"\n",
      "------------------------------\n",
      "Source Node 2 (Similarity Score: 0.6136):\n",
      "Retrieved Chunk: \"Table1. continued\n",
      "Ca...\"\n",
      "------------------------------\n",
      "\n",
      "--- Final RAG Response (using knowledge base) ---\n",
      "The statement is true. The given context information contains the exact quote on page_label: 10, confirming that the NWT indeed suggested caution regarding the proposal to transport polar bear hides to the U.S. on CITES permits and recommended considering whalebone carvings and seal-skin products first.\n"
     ]
    }
   ],
   "source": [
    "# Query about content from the PDF\n",
    "pdf_query = \"Fact check this: <quote> The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears.</quote> If you don't know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Querying the RAG system with:\\n'{pdf_query}'\\n\")\n",
    "\n",
    "# --- Step 1: Query the RAG engine ---\n",
    "# This step internally performs:\n",
    "# 1. Query Embedding: Your 'pdf_query' is converted to a vector.\n",
    "# 2. Retrieval: The vector is used to find the most similar document chunks (Nodes) from your VectorStoreIndex.\n",
    "print(\"--- Processing RAG query... ---\")\n",
    "rag_response_object = query_engine.query(pdf_query)\n",
    "\n",
    "# The 'rag_response_object' now contains both the retrieved nodes and the final synthesized answer.\n",
    "\n",
    "# --- Step 2: Inspect the Retrieved Chunks (Source Nodes) ---\n",
    "print(\"\\n--- Retrieved Context (Source Nodes used by RAG) ---\")\n",
    "if rag_response_object.source_nodes:\n",
    "    for i, source_node in enumerate(rag_response_object.source_nodes):\n",
    "        print(f\"Source Node {i+1} (Similarity Score: {source_node.score:.4f}):\")\n",
    "        # .get_content() is a robust way to get the text from the node.\n",
    "        # .strip() removes leading/trailing whitespace for cleaner printing.\n",
    "        print(f\"Retrieved Chunk: \\\"{source_node.node.get_content().strip()[:20]}...\\\"\")\n",
    "        # You can also print other metadata if available, e.g., source_node.node.metadata\n",
    "        # print(f\"Metadata: {source_node.node.metadata}\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"No source nodes were retrieved for this query.\")\n",
    "\n",
    "# --- Step 3: See the Final LLM Response (Synthesized with RAG) ---\n",
    "# This is the answer generated by the LLM based on your query AND the retrieved chunks.\n",
    "print(\"\\n--- Final RAG Response (using knowledge base) ---\")\n",
    "print(rag_response_object.response) # .response attribute holds the textual answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7375efd",
   "metadata": {},
   "source": [
    "### Compare results\n",
    "\n",
    "To highlight the effectiveness of RAG, the following code compares responses from the RAG system against direct LLM responses without any document context. This comparison demonstrates how RAG provides more accurate, grounded answers for domain-specific questions by leveraging the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd55f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Fact check this: <quote> The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears.</quote> **Important: if you dont the answer just reply 'SORRY, I DONT KNOW' without any other text.** If you do have data to answer, provide full answer quoting the sources\n",
      "\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "SORRY, I DONT KNOW\n",
      "--- RAG Response (using knowledge base) ---\n",
      "\"The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears.\" is TRUE according to page_label: 10.\n"
     ]
    }
   ],
   "source": [
    "# Let's fact check the same query using the direct LLM response without RAG\n",
    "pdf_query = \"Fact check this: <quote> The NWT suggested caution regarding a proposal that polar bear hides be transportable to the U.S. on CITES permits. It was suggested that whalebone carvings and seal-skin products be considered first and then if there are no political problems, possibly consider polar bears.</quote> **Important: if you don't know the answer just reply 'SORRY, I DON'T KNOW' without any other text.** If you do have data to answer, provide full answer quoting the sources\"\n",
    "print(f\"Query: {pdf_query}\\n\")\n",
    "\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(pdf_query)\n",
    "print(direct_response)\n",
    "\n",
    "print(\"--- RAG Response (using knowledge base) ---\")\n",
    "rag_response = query_engine.query(pdf_query)\n",
    "print(f\"{rag_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b52358",
   "metadata": {},
   "source": [
    "Continue testing queries against the knowledge base to evaluate how well the RAG system retrieves and grounds answers using the PDF document. \n",
    "\n",
    "This demonstrates the effectiveness of retrieval-augmented generation compared to direct LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f83c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What does the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue says about the CHC levels? IMPORTANT: If you don't know, say 'I don't know'.\n",
      "\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "I don't know the specific details about what the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue says about the CHC levels. If you're looking for accurate information on this topic, I recommend consulting the original research or a reliable scientific summary.\n",
      "--- RAG Response (using knowledge base) ---\n",
      "The levels of CHCs were generally inversely correlated to latitude, and reanalysis of polar bear fat samples showed that the level of most CHCs, especially chlordane compounds, had increased from 1969 to 1984 in Hudson Bay and Baffin Bay bears.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query about a specific technical detail in the paper\n",
    "technical_query = \"What does the Toxicology and Monitoring of Pollutant Levels in Polar Bear Tissue say about the CHC levels? IMPORTANT: If you don't know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Query: {technical_query}\\n\")\n",
    "\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(technical_query)\n",
    "print(direct_response)\n",
    "\n",
    "print(\"--- RAG Response (using knowledge base) ---\")\n",
    "rag_response = query_engine.query(technical_query)\n",
    "print(f\"{rag_response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47dc7183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who are the authors of the Polar Bear Paper? IMPORTANT: If you don't know, say 'I don't know'.\n",
      "\n",
      "--- Direct LLM Response (without RAG) ---\n",
      "I don't know.\n",
      "--- RAG Response (using knowledge base) ---\n",
      "Steven C. Amstrup and Oystein Wiig are the compilers and editors of the Polar Bear publication, as indicated on page 3. However, the authors listed in the references on page 29 include Stirling, Schweinsburg, Kolenosky, Juniper, Robertson, Luttich, Calvelt, Sjare, Taylor, Bunnell, DeMaster, and Smith. Without more information, it's unclear if they are authors of the Polar Bear Paper or just cited references. Therefore, a more accurate answer would be that Steven C. Amstrup and Oystein Wiig are the compilers and editors, while the other names appear as authors of cited references.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query about authors and publication details\n",
    "authors_query = \"Who are the authors of the Polar Bear Paper? IMPORTANT: If you don't know, say 'I don't know'.\"\n",
    "\n",
    "print(f\"Query: {authors_query}\\n\")\n",
    "print(\"--- Direct LLM Response (without RAG) ---\")\n",
    "direct_response = get_direct_llm_response(authors_query)\n",
    "print(direct_response)\n",
    "\n",
    "print(\"--- RAG Response (using knowledge base) ---\")\n",
    "rag_response = query_engine.query(authors_query)\n",
    "print(f\"{rag_response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7718f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a RAG system using LlamaIndex and kluster.ai, incorporating a PDF document as a knowledge source. Key takeaways include:\n",
    "\n",
    "1. **Embeddings functionality**: Generated and visualized embeddings using the BAAI/bge-m3 model.\n",
    "2. **Batch processing benefits**: Demonstrated how batch embeddings provide significant performance improvements.\n",
    "3. **PDF integration**: Loaded and processed a research paper for the knowledge base, with LlamaIndex handling embedding batching automatically.\n",
    "4. **RAG vs. direct LLM comparison**: Compared responses from the RAG system to direct LLM outputs.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Try with different PDF documents or document types.\n",
    "- Experiment with different chunking strategies to optimize retrieval.\n",
    "- Explore other embedding models available on kluster.ai."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}